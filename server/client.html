<!DOCTYPE html>
<html>
<head>
  <title>Voice Streaming Test</title>
</head>
<body>
  <h1>WebSocket ‚Üî gRPC Voice Streaming</h1>
  <button id="start">Start Mic</button>
  <button id="stop" disabled>Stop</button>
  <div id="status">Disconnected</div>

  <script>
    let ws = null;
    let audioCtx = null;
    let mediaStream;
    let audioWorkletNode;
    let analyser; // Force audio processing
    let isRecording = false;
    
    // Audio playback queue
    let playbackQueue = [];
    let isPlaying = false;

    function connectWebSocket() {
      return new Promise((resolve, reject) => {
        ws = new WebSocket('ws://localhost:80/ws');
        ws.binaryType = 'arraybuffer';
        
        ws.onopen = () => {
          console.log('‚úÖ WebSocket Connected');
          document.getElementById('status').textContent = 'Connected';
          resolve();
        };
        
        ws.onclose = () => {
          console.log('‚ùå WebSocket Disconnected');
          document.getElementById('status').textContent = 'Disconnected';
        };
        
        ws.onerror = (err) => {
          console.error('‚ùå WebSocket Error:', err);
          reject(err);
        };

        // Receive ServerAudioChunk from gRPC server and playback
        ws.onmessage = async (e) => {
          if (e.data instanceof ArrayBuffer) {
            console.log('‚úÖ Received audio chunk from server:', e.data.byteLength, 'bytes');
            
            // Initialize audioCtx if not yet created
            if (!audioCtx) {
              audioCtx = new AudioContext({ sampleRate: 16000 });
              console.log('üéµ AudioContext created on first message');
            }
            
            // Convert Int16 PCM to Float32 for Web Audio API
            const int16Array = new Int16Array(e.data);
            
            const float32Array = new Float32Array(int16Array.length);
            for (let i = 0; i < int16Array.length; i++) {
              float32Array[i] = int16Array[i] / 32768.0; // normalize to [-1, 1]
            }
            
            // Create audio buffer and play
            const audioBuffer = audioCtx.createBuffer(1, float32Array.length, 16000);
            audioBuffer.getChannelData(0).set(float32Array);
            
            const samples = audioBuffer.getChannelData(0);
            console.log('üìä Audio buffer created - duration:', (audioBuffer.duration * 1000).toFixed(0), 'ms');
            
            playbackQueue.push(audioBuffer);
            if (!isPlaying) {
              playNext();
            }
          }
        };
      });
    }
    
    function playNext() {
      if (playbackQueue.length === 0) {
        isPlaying = false;
        console.log('‚è∏  Playback queue empty');
        return;
      }
      
      isPlaying = true;
      const buffer = playbackQueue.shift();
      
      console.log('‚ñ∂Ô∏è  Playing audio - duration:', (buffer.duration * 1000).toFixed(0), 'ms, queue remaining:', playbackQueue.length);
      
      try {
        const source = audioCtx.createBufferSource();
        source.buffer = buffer;
        source.connect(audioCtx.destination);
        
        source.onended = () => {
          console.log('‚úÖ Audio playback completed');
          playNext();
        };
        
        source.start(0);
        
      } catch (err) {
        console.error('‚ùå Playback error:', err);
        isPlaying = false;
      }
    }

    document.getElementById('start').onclick = async () => {
      try {
        document.getElementById('status').textContent = 'Connecting...';
        
        // Initialize audio context
        if (!audioCtx) {
          audioCtx = new AudioContext({ sampleRate: 16000 });
        }
        
        // Connect WebSocket
        await connectWebSocket();
        
        // Load AudioWorklet module
        await audioCtx.audioWorklet.addModule('audio-processor.js');
        
        // Request microphone access
        mediaStream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true
          } 
        });
        
        console.log('üé§ Microphone connected');
        
        const source = audioCtx.createMediaStreamSource(mediaStream);
        console.log('‚úÖ MediaStreamSource created');
        
        // Create AudioWorklet node
        audioWorkletNode = new AudioWorkletNode(audioCtx, 'audio-capture-processor');
        
        // Receive PCM data from worklet and send to server
        audioWorkletNode.port.onmessage = (e) => {
          if (!isRecording) return;
          
          // Send raw binary PCM data via WebSocket
          if (ws && ws.readyState === WebSocket.OPEN && e.data.audio && e.data.audio.byteLength > 0) {
            console.log('üì§ Sending audio to server:', e.data.audio.byteLength, 'bytes');
            ws.send(e.data.audio);
          }
        };
        
        source.connect(audioWorkletNode);
        
        // Force audio processing by connecting to analyser
        analyser = audioCtx.createAnalyser();
        audioWorkletNode.connect(analyser);
        analyser.connect(audioCtx.destination);
        
        isRecording = true;
        document.getElementById('start').disabled = true;
        document.getElementById('stop').disabled = false;
        document.getElementById('status').textContent = 'Recording';
        console.log('üé§ Recording started');
        
      } catch (err) {
        console.error('‚ùå Error:', err);
        document.getElementById('status').textContent = 'Error: ' + err.message;
        alert('Cannot access microphone: ' + err.message);
      }
    };
    
    document.getElementById('stop').onclick = () => {
      isRecording = false;
      
      if (audioWorkletNode) {
        audioWorkletNode.disconnect();
        audioWorkletNode = null;
      }
      
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      
      if (ws) {
        ws.close();
        ws = null;
      }
      
      document.getElementById('start').disabled = false;
      document.getElementById('stop').disabled = true;
      document.getElementById('status').textContent = 'Stopped';
      console.log('‚èπ  Recording stopped');
    };
    
    // Test function: generate and play test tone
    window.testTone = () => {
      try {
        if (!audioCtx) {
          audioCtx = new AudioContext({ sampleRate: 16000 });
        }
        
        console.log('üîä Playing test tone (440Hz)...');
        
        const sampleRate = audioCtx.sampleRate;
        const frequency = 440;
        const duration = 1;
        const numSamples = sampleRate * duration;
        
        const buffer = audioCtx.createBuffer(1, numSamples, sampleRate);
        const channelData = buffer.getChannelData(0);
        
        for (let i = 0; i < numSamples; i++) {
          channelData[i] = Math.sin(2 * Math.PI * frequency * i / sampleRate) * 0.3;
        }
        
        const source = audioCtx.createBufferSource();
        source.buffer = buffer;
        source.connect(audioCtx.destination);
        source.start(0);
        
        source.onended = () => {
          console.log('‚úÖ Test tone finished');
        };
        
      } catch (err) {
        console.error('‚ùå Test tone error:', err);
      }
    };
  </script>
</body>
</html>