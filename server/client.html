<!DOCTYPE html>
<html>
<head>
  <title>Voice Streaming Test</title>
</head>
<body>
  <h1>WebSocket ‚Üî gRPC Voice Streaming</h1>
  <button id="start">Start Mic</button>
  <button id="stop" disabled>Stop</button>
  <div id="status">Disconnected</div>

  <div style="margin-top: 15px; padding: 10px; border: 1px solid #ccc; border-radius: 5px; max-width: 400px;">
    <div style="margin-bottom: 5px; font-weight: bold;">Microphone Volume:</div>
    <div style="width: 100%; height: 20px; background-color: #f0f0f0; border-radius: 10px; overflow: hidden; border: 1px solid #ddd;">
      <div id="mic-level" style="width: 0%; height: 100%; background-color: #4CAF50; transition: width 0.05s linear;"></div>
    </div>
  </div>

  <script>
    let ws = null;
    let audioCtx = null;
    let mediaStream;
    let audioWorkletNode;
    let analyser; // Force audio processing
    let isRecording = false;

    let nextStartTime = 0;
    const PLAYBACK_SAMPLE_RATE = 24000; // Default for many AI voices

    function connectWebSocket() {
      return new Promise((resolve, reject) => {
        // Auto-detect protocol (ws or wss) and host based on current page URL
        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const host = 'voice-bot-ws.knmholdings.org'; // e.g. localhost:8080 or my-domain.com
        const token = '3k659sdg98gkn3d9ghhg977';
        const phone = '84987654321';
        const callId = 'call123';
        // Construct the WebSocket URL dynamically
        const wsUrl = `${protocol}//${host}/live-call/websocket/${callId}/?token=${token}`;
        console.log('Connecting to:', wsUrl);

        ws = new WebSocket(wsUrl);
        ws.binaryType = 'arraybuffer';
        
        ws.onopen = () => {
          console.log('‚úÖ WebSocket Connected');
          document.getElementById('status').textContent = 'Connected';
          resolve();
        };
        
        ws.onclose = () => {
          console.log('‚ùå WebSocket Disconnected');
          document.getElementById('status').textContent = 'Disconnected';
        };
        
        ws.onerror = (event) => {
          console.error('‚ùå WebSocket Error:', event);
          reject(new Error('WebSocket connection failed. Please check if the server is running at ws://localhost:80/ws'));
        };

        // Receive ServerAudioChunk from gRPC server
        ws.onmessage = async (e) => {
          if (e.data instanceof ArrayBuffer) {
            console.log('‚úÖ Received audio chunk from server:', e.data.byteLength, 'bytes');
            
            if (!audioCtx) return;

            try {
              // Convert Int16 PCM to Float32
              const int16Data = new Int16Array(e.data);
              const float32Data = new Float32Array(int16Data.length);
              
              for (let i = 0; i < int16Data.length; i++) {
                // Normalize to [-1.0, 1.0]
                float32Data[i] = int16Data[i] / 32768.0;
              }

              // Create AudioBuffer
              const audioBuffer = audioCtx.createBuffer(1, float32Data.length, PLAYBACK_SAMPLE_RATE);
              audioBuffer.getChannelData(0).set(float32Data);

              // Create Source
              const source = audioCtx.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(audioCtx.destination);

              // Schedule playback
              // Ensure we don't schedule in the past, but also try to keep it continuous
              const currentTime = audioCtx.currentTime;
              if (nextStartTime < currentTime) {
                nextStartTime = currentTime + 0.05; // Add a small buffer if we fell behind
              }
              
              source.start(nextStartTime);
              nextStartTime += audioBuffer.duration;
              
            } catch (err) {
              console.error('Error playing audio chunk:', err);
            }
          }
        };
      });
    }

    function visualizeVolume() {
      if (!isRecording || !analyser) return;
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      analyser.getByteFrequencyData(dataArray);
      
      // Calculate average volume
      let sum = 0;
      for(let i = 0; i < dataArray.length; i++) {
        sum += dataArray[i];
      }
      const average = sum / dataArray.length;
      
      // Scale for better visibility (0-255 range, but usually lower)
      // Multiply by 3 to make normal speech visible around 50%
      const volume = Math.min(100, average * 3);
      
      const level = document.getElementById('mic-level');
      if (level) {
        level.style.width = volume + '%';
        
        // Change color based on volume
        if (volume > 80) level.style.backgroundColor = '#ff4444'; // Red for clipping/loud
        else if (volume > 50) level.style.backgroundColor = '#ffbb33'; // Yellow for good level
        else level.style.backgroundColor = '#4CAF50'; // Green for low/normal
      }
      
      requestAnimationFrame(visualizeVolume);
    }

    document.getElementById('start').onclick = async () => {
      try {
        document.getElementById('status').textContent = 'Connecting...';
        
        // Initialize audio context
        if (!audioCtx) {
          audioCtx = new AudioContext();
          console.log('üéµ AudioContext created. Sample rate:', audioCtx.sampleRate);
        }
        
        // Connect WebSocket
        await connectWebSocket();
        
        // Load AudioWorklet module
        await audioCtx.audioWorklet.addModule('audio-processor.js');
        
        // Request microphone access with noise suppression and echo cancellation
        // Browser automatically uses these if supported
        const constraints = {
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: false // Disable auto gain to preserve volume accuracy
          }
        };
        
        try {
          mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = mediaStream.getAudioTracks()[0];
          console.log('üé§ Microphone connected:', track.label);
          
          // Check if audio processing is actually enabled
          const settings = track.getSettings();
          console.log('üîß Audio Processing:', {
            echoCancellation: settings.echoCancellation || false,
            noiseSuppression: settings.noiseSuppression || false,
            autoGainControl: settings.autoGainControl || false
          });
        } catch (err) {
          // If constraints fail, fallback to basic audio
          console.warn('‚ö†Ô∏è  Audio constraints not supported, using basic mic:', err.message);
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          const track = mediaStream.getAudioTracks()[0];
          console.log('üé§ Microphone connected (basic):', track.label);
        }
        
        const source = audioCtx.createMediaStreamSource(mediaStream);
        console.log('‚úÖ MediaStreamSource created');
        
        // Create AudioWorklet node
        audioWorkletNode = new AudioWorkletNode(audioCtx, 'audio-capture-processor');
        
        // Receive PCM data from worklet and send to server
        audioWorkletNode.port.onmessage = (e) => {
          if (!isRecording) return;
          
          // Send raw binary PCM data via WebSocket
          if (ws && ws.readyState === WebSocket.OPEN && e.data.audio && e.data.audio.byteLength > 0) {
            console.log('üì§ Sending audio to server:', e.data.audio.byteLength, 'bytes');
            ws.send(e.data.audio);
          }
        };
        
        source.connect(audioWorkletNode);
        
        // Create analyser for volume visualization (not connected to speaker)
        analyser = audioCtx.createAnalyser();
        analyser.fftSize = 256; // Smaller FFT size for faster response
        audioWorkletNode.connect(analyser);
        
        isRecording = true;
        visualizeVolume(); // Start visualization loop

        document.getElementById('start').disabled = true;
        document.getElementById('stop').disabled = false;
        document.getElementById('status').textContent = 'Recording';
        console.log('üé§ Recording started');
        
      } catch (err) {
        console.error('‚ùå Error:', err);
        const msg = err.message || err.toString() || 'Unknown error';
        document.getElementById('status').textContent = 'Error: ' + msg;
        alert('Error: ' + msg);
      }
    };
    
    document.getElementById('stop').onclick = () => {
      isRecording = false;
      
      // Reset volume meter
      const level = document.getElementById('mic-level');
      if (level) {
        level.style.width = '0%';
        level.style.backgroundColor = '#4CAF50';
      }
      
      if (audioWorkletNode) {
        audioWorkletNode.disconnect();
        audioWorkletNode = null;
      }
      
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      
      if (audioCtx) {
        audioCtx.close();
        audioCtx = null;
      }
      
      if (ws) {
        ws.close();
        ws = null;
      }
      
      document.getElementById('start').disabled = false;
      document.getElementById('stop').disabled = true;
      document.getElementById('status').textContent = 'Stopped';
      console.log('‚èπ  Recording stopped');
    };
    
    // Test function: generate and play test tone
    window.testTone = () => {
      try {
        if (!audioCtx) {
          audioCtx = new AudioContext({ sampleRate: 16000 });
        }
        
        console.log('üîä Playing test tone (440Hz)...');
        
        const sampleRate = audioCtx.sampleRate;
        const frequency = 440;
        const duration = 1;
        const numSamples = sampleRate * duration;
        
        const buffer = audioCtx.createBuffer(1, numSamples, sampleRate);
        const channelData = buffer.getChannelData(0);
        
        for (let i = 0; i < numSamples; i++) {
          channelData[i] = Math.sin(2 * Math.PI * frequency * i / sampleRate) * 0.3;
        }
        
        const source = audioCtx.createBufferSource();
        source.buffer = buffer;
        source.connect(audioCtx.destination);
        source.start(0);
        
        source.onended = () => {
          console.log('‚úÖ Test tone finished');
        };
        
      } catch (err) {
        console.error('‚ùå Test tone error:', err);
      }
    };
  </script>
</body>
</html>