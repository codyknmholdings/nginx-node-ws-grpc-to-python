<!DOCTYPE html>
<html>
<head>
  <title>Voice Streaming Test</title>
</head>
<body>
  <h1>WebSocket ‚Üî gRPC Voice Streaming</h1>
  <button id="start">Start Mic</button>
  <button id="stop" disabled>Stop</button>
  <div id="status">Disconnected</div>

  <div style="margin-top: 15px; padding: 10px; border: 1px solid #ccc; border-radius: 5px; max-width: 400px;">
    <div style="margin-bottom: 5px; font-weight: bold;">Microphone Volume:</div>
    <div style="width: 100%; height: 20px; background-color: #f0f0f0; border-radius: 10px; overflow: hidden; border: 1px solid #ddd;">
      <div id="mic-level" style="width: 0%; height: 100%; background-color: #4CAF50; transition: width 0.05s linear;"></div>
    </div>
  </div>

  <script>
    let ws = null;
    let audioCtx = null;
    let mediaStream;
    let audioWorkletNode;
    let analyser; // Force audio processing
    let isRecording = false;

    let nextStartTime = 0;
    // const PLAYBACK_SAMPLE_RATE = 24000; // REMOVED: Will use dynamic sample rate from server

    function connectWebSocket() {
      return new Promise((resolve, reject) => {
        // Auto-detect protocol (ws or wss) and host based on current page URL
        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const host = window.location.host; // e.g. localhost:8080 or my-domain.com
        
        // Connection parameters
        const tenantId = 'tenant_001';
        const hotline = '1900123456';
        const phone = '84987654321';
        const callId = 'call_' + Date.now();
        const speakerId = 'speaker_001';
        const env = 'dev';
        
        // Construct the WebSocket URL: /api/asr/streaming?tenant_id=...&hotline=...&phone=...&call_id=...&speaker_id=...&env=...
        const wsUrl = `${protocol}//${host}/api/asr/streaming?tenant_id=${tenantId}&hotline=${hotline}&phone=${phone}&call_id=${callId}&speaker_id=${speakerId}&env=${env}`;
        console.log('Connecting to:', wsUrl);

        ws = new WebSocket(wsUrl);
        // ws.binaryType = 'arraybuffer'; // We are using JSON now
        
        ws.onopen = () => {
          console.log('‚úÖ WebSocket Connected');
          document.getElementById('status').textContent = 'Connected';
          // Server auto-sends initial_info to gRPC, no need to send from client
          resolve();
        };
        
        ws.onclose = () => {
          console.log('‚ùå WebSocket Disconnected');
          document.getElementById('status').textContent = 'Disconnected';
          // Stop recording if connection drops
          if (isRecording) {
             document.getElementById('stop').click();
          }
        };
        
        ws.onerror = (event) => {
          console.error('‚ùå WebSocket Error:', event);
          reject(new Error('WebSocket connection failed. Please check if the server is running at ' + wsUrl));
        };

        // Receive messages from server (new format)
        ws.onmessage = async (e) => {
          try {
            const message = JSON.parse(e.data);
            
            // Handle playAudio message
            if (message.type === 'playAudio') {
              const audioData = message.data;
              const base64Audio = audioData.audioContent;
              
              // Decode Base64 to ArrayBuffer
              const binaryString = window.atob(base64Audio);
              const len = binaryString.length;
              const bytes = new Uint8Array(len);
              for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
              }
              const pcmData = bytes.buffer;

              const sampleRate = audioData.sampleRate || 8000;
              console.log(`‚úÖ Received playAudio: ${pcmData.byteLength} bytes, Rate: ${sampleRate}Hz, Duration: ${audioData.audioDuration}s`);
              
              if (!audioCtx) return;

              // Convert Int16 PCM to Float32
              const int16Data = new Int16Array(pcmData);
              const float32Data = new Float32Array(int16Data.length);
              
              for (let i = 0; i < int16Data.length; i++) {
                // Normalize to [-1.0, 1.0]
                float32Data[i] = int16Data[i] / 32768.0;
              }

              // Create AudioBuffer
              const audioBuffer = audioCtx.createBuffer(1, float32Data.length, sampleRate);
              audioBuffer.getChannelData(0).set(float32Data);

              // Create Source
              const source = audioCtx.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(audioCtx.destination);

              // Schedule playback
              const currentTime = audioCtx.currentTime;
              if (nextStartTime < currentTime) {
                nextStartTime = currentTime + 0.05;
              }
              
              source.start(nextStartTime);
              nextStartTime += audioBuffer.duration;

            }
            // Handle transfer message
            else if (message.type === 'transfer') {
              console.log('‚Ü™Ô∏è  Transfer signal received, SIP:', message.sip_number);
              alert('Call is being transferred to SIP: ' + message.sip_number);
            }
            // Handle disconnect message
            else if (message.type === 'disconnect') {
              console.log('üì¥ Disconnect signal received from server');
              isRecording = false;
              document.getElementById('stop').click();
            }
            // Handle error message
            else if (message.type === 'error') {
              console.error('‚ùå Server error:', message.data?.message, '(code:', message.data?.error_code, ')');
              alert('Server error: ' + (message.data?.message || 'Unknown error'));
            }
            // Handle unknown message types
            else {
              console.log('‚ÑπÔ∏è Received message:', message);
            }
          } catch (err) {
            console.error('Error processing message:', err);
          }
        };
      });
    }

    function visualizeVolume() {
      if (!isRecording || !analyser) return;
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      analyser.getByteFrequencyData(dataArray);
      
      // Calculate average volume
      let sum = 0;
      for(let i = 0; i < dataArray.length; i++) {
        sum += dataArray[i];
      }
      const average = sum / dataArray.length;
      
      // Scale for better visibility (0-255 range, but usually lower)
      // Multiply by 3 to make normal speech visible around 50%
      const volume = Math.min(100, average * 3);
      
      const level = document.getElementById('mic-level');
      if (level) {
        level.style.width = volume + '%';
        
        // Change color based on volume
        if (volume > 80) level.style.backgroundColor = '#ff4444'; // Red for clipping/loud
        else if (volume > 50) level.style.backgroundColor = '#ffbb33'; // Yellow for good level
        else level.style.backgroundColor = '#4CAF50'; // Green for low/normal
      }
      
      requestAnimationFrame(visualizeVolume);
    }

    document.getElementById('start').onclick = async () => {
      try {
        document.getElementById('status').textContent = 'Connecting...';
        
        // Initialize audio context
        if (!audioCtx) {
          audioCtx = new AudioContext();
          console.log('üéµ AudioContext created. Sample rate:', audioCtx.sampleRate);
        }
        
        // Connect WebSocket
        await connectWebSocket();
        
        // Load AudioWorklet module
        await audioCtx.audioWorklet.addModule('audio-processor.js');
        
        // Request microphone access with noise suppression and echo cancellation
        // Browser automatically uses these if supported
        const constraints = {
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: false // Disable auto gain to preserve volume accuracy
          }
        };
        
        try {
          mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = mediaStream.getAudioTracks()[0];
          console.log('üé§ Microphone connected:', track.label);
          
          // Check if audio processing is actually enabled
          const settings = track.getSettings();
          console.log('üîß Audio Processing:', {
            echoCancellation: settings.echoCancellation || false,
            noiseSuppression: settings.noiseSuppression || false,
            autoGainControl: settings.autoGainControl || false
          });
        } catch (err) {
          // If constraints fail, fallback to basic audio
          console.warn('‚ö†Ô∏è  Audio constraints not supported, using basic mic:', err.message);
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          const track = mediaStream.getAudioTracks()[0];
          console.log('üé§ Microphone connected (basic):', track.label);
        }
        
        const source = audioCtx.createMediaStreamSource(mediaStream);
        console.log('‚úÖ MediaStreamSource created');
        
        // Create AudioWorklet node
        audioWorkletNode = new AudioWorkletNode(audioCtx, 'audio-capture-processor');
        
        // Target sample rate after downsampling in audio-processor.js
        const TARGET_SAMPLE_RATE = 8000;

        // Receive PCM data from worklet and send to server
        audioWorkletNode.port.onmessage = (e) => {
          if (!isRecording) return;
          
          if (ws && ws.readyState === WebSocket.OPEN && e.data.audio && e.data.audio.byteLength > 0) {
            // Convert Float32 to Int16
            const float32Data = new Float32Array(e.data.audio);
            const int16Data = new Int16Array(float32Data.length);
            for (let i = 0; i < float32Data.length; i++) {
                const s = Math.max(-1, Math.min(1, float32Data[i]));
                int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }

            // Send binary audio directly (8kHz, 16-bit, mono)
            console.log('üì§ Sending binary audio:', int16Data.byteLength, 'bytes @', TARGET_SAMPLE_RATE, 'Hz');
            ws.send(int16Data.buffer);
          }
        };
        
        source.connect(audioWorkletNode);
        
        // Create analyser for volume visualization (not connected to speaker)
        analyser = audioCtx.createAnalyser();
        analyser.fftSize = 256; // Smaller FFT size for faster response
        audioWorkletNode.connect(analyser);
        
        isRecording = true;
        visualizeVolume(); // Start visualization loop

        document.getElementById('start').disabled = true;
        document.getElementById('stop').disabled = false;
        document.getElementById('status').textContent = 'Recording';
        console.log('üé§ Recording started');
        
      } catch (err) {
        console.error('‚ùå Error:', err);
        const msg = err.message || err.toString() || 'Unknown error';
        document.getElementById('status').textContent = 'Error: ' + msg;
        alert('Error: ' + msg);
      }
    };
    
    document.getElementById('stop').onclick = () => {
      isRecording = false;
      
      // Send disconnect message to server before closing WebSocket (doc v0.3 format)
      if (ws && ws.readyState === WebSocket.OPEN) {
        const disconnectMessage = {
          disconnect: {}
        };
        console.log('üì§ Sending disconnect message');
        ws.send(JSON.stringify(disconnectMessage));
      }
      
      // Reset volume meter
      const level = document.getElementById('mic-level');
      if (level) {
        level.style.width = '0%';
        level.style.backgroundColor = '#4CAF50';
      }
      
      if (audioWorkletNode) {
        audioWorkletNode.disconnect();
        audioWorkletNode = null;
      }
      
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      
      if (audioCtx) {
        audioCtx.close();
        audioCtx = null;
      }
      
      if (ws) {
        ws.close();
        ws = null;
      }
      
      document.getElementById('start').disabled = false;
      document.getElementById('stop').disabled = true;
      document.getElementById('status').textContent = 'Stopped';
      console.log('‚èπ  Recording stopped');
    };
  </script>
</body>
</html>