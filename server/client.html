<!DOCTYPE html>
<html>
<head>
  <title>Voice Streaming Test</title>
</head>
<body>
  <h1>WebSocket ‚Üî gRPC Voice Streaming</h1>
  <button id="start">Start Mic</button>
  <button id="stop" disabled>Stop</button>
  <div id="status">Disconnected</div>

  <div style="margin-top: 15px; padding: 10px; border: 1px solid #ccc; border-radius: 5px; max-width: 400px;">
    <div style="margin-bottom: 5px; font-weight: bold;">Microphone Volume:</div>
    <div style="width: 100%; height: 20px; background-color: #f0f0f0; border-radius: 10px; overflow: hidden; border: 1px solid #ddd;">
      <div id="mic-level" style="width: 0%; height: 100%; background-color: #4CAF50; transition: width 0.05s linear;"></div>
    </div>
  </div>

  <script>
    let ws = null;
    let audioCtx = null;
    let mediaStream;
    let audioWorkletNode;
    let analyser; // Force audio processing
    let isRecording = false;

    let nextStartTime = 0;
    // const PLAYBACK_SAMPLE_RATE = 24000; // REMOVED: Will use dynamic sample rate from server

    function connectWebSocket() {
      return new Promise((resolve, reject) => {
        // Auto-detect protocol (ws or wss) and host based on current page URL
        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const host = window.location.host; // e.g. localhost:8080 or my-domain.com
        const token = '3k659sdg98gkn3d9ghhg977';
        const phone = '84987654321';
        const callId = 'call_' + Date.now();
        const tenantId = 'tenant_001';
        const speakerId = 'speaker_001';
        const env = 'dev';
        // Construct the WebSocket URL dynamically
        const wsUrl = `${protocol}//${host}/live-call/websocket/${callId}/${phone}?token=${token}&tenant_id=${tenantId}&speaker_id=${speakerId}&env=${env}`;
        console.log('Connecting to:', wsUrl);

        ws = new WebSocket(wsUrl);
        // ws.binaryType = 'arraybuffer'; // We are using JSON now
        
        ws.onopen = () => {
          console.log('‚úÖ WebSocket Connected');
          document.getElementById('status').textContent = 'Connected';
          resolve();
        };
        
        ws.onclose = () => {
          console.log('‚ùå WebSocket Disconnected');
          document.getElementById('status').textContent = 'Disconnected';
          // Stop recording if connection drops
          if (isRecording) {
             document.getElementById('stop').click();
          }
        };
        
        ws.onerror = (event) => {
          console.error('‚ùå WebSocket Error:', event);
          reject(new Error('WebSocket connection failed. Please check if the server is running at ' + wsUrl));
        };

        // Receive ws_audio_output from gRPC server
        ws.onmessage = async (e) => {
          try {
            const message = JSON.parse(e.data);
            
            if (message.ws_audio_output) {
              const base64Audio = message.ws_audio_output.audio_content;
              // Decode Base64 to ArrayBuffer
              const binaryString = window.atob(base64Audio);
              const len = binaryString.length;
              const bytes = new Uint8Array(len);
              for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
              }
              const audioData = bytes.buffer;

              console.log('‚úÖ Received audio chunk from server:', audioData.byteLength, 'bytes');
              
              if (!audioCtx) return;

              // Convert Int16 PCM to Float32
              const int16Data = new Int16Array(audioData);
              const float32Data = new Float32Array(int16Data.length);
              
              for (let i = 0; i < int16Data.length; i++) {
                // Normalize to [-1.0, 1.0]
                float32Data[i] = int16Data[i] / 32768.0;
              }

              // Create AudioBuffer
              const sampleRate = message.ws_audio_output.sample_rate || 16000;
              console.log(`‚úÖ Received audio chunk: ${audioData.byteLength} bytes, Rate: ${sampleRate}Hz`);
              const audioBuffer = audioCtx.createBuffer(1, float32Data.length, sampleRate);
              audioBuffer.getChannelData(0).set(float32Data);

              // Create Source
              const source = audioCtx.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(audioCtx.destination);

              // Schedule playback
              const currentTime = audioCtx.currentTime;
              if (nextStartTime < currentTime) {
                nextStartTime = currentTime + 0.05;
              }
              
              source.start(nextStartTime);
              nextStartTime += audioBuffer.duration;

            } else if (message.ws_signal) {
              console.log('‚ÑπÔ∏è Received signal:', message.ws_signal);
              if (message.ws_signal.end_call) {
                console.log('üì¥ Call ended by server. Call ID:', message.ws_signal.end_call.call_id);
                // Stop recording
                isRecording = false;
                document.getElementById('stop').click();
              } else if (message.ws_signal.transfer_call) {
                console.log('‚Ü™Ô∏è  Call transfer signal received');
                const transferInfo = message.ws_signal.transfer_call;
                console.log('   Target staff:', transferInfo.target_staff);
                alert('Call is being transferred to: ' + (transferInfo.target_staff?.map(s => s.name).join(', ') || 'available staff'));
              }
            }
          } catch (err) {
            console.error('Error processing message:', err);
          }
        };
      });
    }

    function visualizeVolume() {
      if (!isRecording || !analyser) return;
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      analyser.getByteFrequencyData(dataArray);
      
      // Calculate average volume
      let sum = 0;
      for(let i = 0; i < dataArray.length; i++) {
        sum += dataArray[i];
      }
      const average = sum / dataArray.length;
      
      // Scale for better visibility (0-255 range, but usually lower)
      // Multiply by 3 to make normal speech visible around 50%
      const volume = Math.min(100, average * 3);
      
      const level = document.getElementById('mic-level');
      if (level) {
        level.style.width = volume + '%';
        
        // Change color based on volume
        if (volume > 80) level.style.backgroundColor = '#ff4444'; // Red for clipping/loud
        else if (volume > 50) level.style.backgroundColor = '#ffbb33'; // Yellow for good level
        else level.style.backgroundColor = '#4CAF50'; // Green for low/normal
      }
      
      requestAnimationFrame(visualizeVolume);
    }

    document.getElementById('start').onclick = async () => {
      try {
        document.getElementById('status').textContent = 'Connecting...';
        
        // Initialize audio context
        if (!audioCtx) {
          audioCtx = new AudioContext();
          console.log('üéµ AudioContext created. Sample rate:', audioCtx.sampleRate);
        }
        
        // Connect WebSocket
        await connectWebSocket();
        
        // Load AudioWorklet module
        await audioCtx.audioWorklet.addModule('audio-processor.js');
        
        // Request microphone access with noise suppression and echo cancellation
        // Browser automatically uses these if supported
        const constraints = {
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: false // Disable auto gain to preserve volume accuracy
          }
        };
        
        try {
          mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = mediaStream.getAudioTracks()[0];
          console.log('üé§ Microphone connected:', track.label);
          
          // Check if audio processing is actually enabled
          const settings = track.getSettings();
          console.log('üîß Audio Processing:', {
            echoCancellation: settings.echoCancellation || false,
            noiseSuppression: settings.noiseSuppression || false,
            autoGainControl: settings.autoGainControl || false
          });
        } catch (err) {
          // If constraints fail, fallback to basic audio
          console.warn('‚ö†Ô∏è  Audio constraints not supported, using basic mic:', err.message);
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          const track = mediaStream.getAudioTracks()[0];
          console.log('üé§ Microphone connected (basic):', track.label);
        }
        
        const source = audioCtx.createMediaStreamSource(mediaStream);
        console.log('‚úÖ MediaStreamSource created');
        
        // Create AudioWorklet node
        audioWorkletNode = new AudioWorkletNode(audioCtx, 'audio-capture-processor');
        
        // Receive PCM data from worklet and send to server
        audioWorkletNode.port.onmessage = (e) => {
          if (!isRecording) return;
          
          if (ws && ws.readyState === WebSocket.OPEN && e.data.audio && e.data.audio.byteLength > 0) {
            // Convert Float32 to Int16
            const float32Data = new Float32Array(e.data.audio);
            const int16Data = new Int16Array(float32Data.length);
            for (let i = 0; i < float32Data.length; i++) {
                const s = Math.max(-1, Math.min(1, float32Data[i]));
                int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }

            // Convert Int16 buffer to Base64
            let binary = '';
            const bytes = new Uint8Array(int16Data.buffer);
            const len = bytes.byteLength;
            for (let i = 0; i < len; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            const base64Audio = window.btoa(binary);
            const sampleRate = audioCtx.sampleRate;

            const message = {
                ws_audio_input: {
                    sample_rate: sampleRate,
                    sample_width: 16,
                    num_channels: 1,
                    duration: int16Data.length / sampleRate,
                    audio_content: base64Audio
                }
            };

            console.log('üì§ Sending audio chunk:', int16Data.byteLength, 'bytes');
            ws.send(JSON.stringify(message));
          }
        };
        
        source.connect(audioWorkletNode);
        
        // Create analyser for volume visualization (not connected to speaker)
        analyser = audioCtx.createAnalyser();
        analyser.fftSize = 256; // Smaller FFT size for faster response
        audioWorkletNode.connect(analyser);
        
        isRecording = true;
        visualizeVolume(); // Start visualization loop

        document.getElementById('start').disabled = true;
        document.getElementById('stop').disabled = false;
        document.getElementById('status').textContent = 'Recording';
        console.log('üé§ Recording started');
        
      } catch (err) {
        console.error('‚ùå Error:', err);
        const msg = err.message || err.toString() || 'Unknown error';
        document.getElementById('status').textContent = 'Error: ' + msg;
        alert('Error: ' + msg);
      }
    };
    
    document.getElementById('stop').onclick = () => {
      isRecording = false;
      
      // Send disconnect message to server before closing WebSocket
      if (ws && ws.readyState === WebSocket.OPEN) {
        const disconnectMessage = {
          ws_disconnect: {}
        };
        console.log('üì§ Sending disconnect message');
        ws.send(JSON.stringify(disconnectMessage));
      }
      
      // Reset volume meter
      const level = document.getElementById('mic-level');
      if (level) {
        level.style.width = '0%';
        level.style.backgroundColor = '#4CAF50';
      }
      
      if (audioWorkletNode) {
        audioWorkletNode.disconnect();
        audioWorkletNode = null;
      }
      
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      
      if (audioCtx) {
        audioCtx.close();
        audioCtx = null;
      }
      
      if (ws) {
        ws.close();
        ws = null;
      }
      
      document.getElementById('start').disabled = false;
      document.getElementById('stop').disabled = true;
      document.getElementById('status').textContent = 'Stopped';
      console.log('‚èπ  Recording stopped');
    };
    
    // Test function: generate and play test tone
    window.testTone = () => {
      try {
        if (!audioCtx) {
          audioCtx = new AudioContext({ sampleRate: 16000 });
        }
        
        console.log('üîä Playing test tone (440Hz)...');
        
        const sampleRate = audioCtx.sampleRate;
        const frequency = 440;
        const duration = 1;
        const numSamples = sampleRate * duration;
        
        const buffer = audioCtx.createBuffer(1, numSamples, sampleRate);
        const channelData = buffer.getChannelData(0);
        
        for (let i = 0; i < numSamples; i++) {
          channelData[i] = Math.sin(2 * Math.PI * frequency * i / sampleRate) * 0.3;
        }
        
        const source = audioCtx.createBufferSource();
        source.buffer = buffer;
        source.connect(audioCtx.destination);
        source.start(0);
        
        source.onended = () => {
          console.log('‚úÖ Test tone finished');
        };
        
      } catch (err) {
        console.error('‚ùå Test tone error:', err);
      }
    };
  </script>
</body>
</html>